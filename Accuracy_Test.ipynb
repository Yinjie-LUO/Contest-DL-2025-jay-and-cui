{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Use Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install \"unsloth@git+https://github.com/unslothai/unsloth.git@September-2025-v3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KdmzYz1cFM8z",
    "outputId": "4337d4eb-e873-40b2-dcb7-acc68be41214"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import sys\n",
    "import os\n",
    "drive.mount('/content/drive')\n",
    "sys.path.append('/content/drive/MyDrive/unsloth_env')\n",
    "os.environ[\"HF_HOME\"] = \"/content/drive/MyDrive/hf_cache\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/content/drive/MyDrive/hf_cache\"\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"/content/drive/MyDrive/hf_cache\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Use [NYUAD HPC](https://ood.hpc.abudhabi.nyu.edu/pun/sys/dashboard/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!module avail gcc\n",
    "!module avail g++\n",
    "!module load gcc/9.2.0\n",
    "!module show gcc/9.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "gcc_bin = \"/share/apps/NYUAD5/gcc/9.2.0/bin\"\n",
    "os.environ[\"CC\"] = os.path.join(gcc_bin, \"gcc\")\n",
    "os.environ[\"CXX\"] = os.path.join(gcc_bin, \"g++\")\n",
    "os.environ[\"PATH\"] = f\"{gcc_bin}:{os.environ.get('PATH', '')}\"\n",
    "\n",
    "print(\"CC =\", os.environ[\"CC\"])\n",
    "print(\"CXX =\", os.environ[\"CXX\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "print(\"Number of GPUs:\", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dOQWVBbCFvTQ",
    "outputId": "8e392654-648f-4583-f761-2e243128cda5"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# this is the model-saving path, where the model checkpoint is saved\n",
    "save_path = \"/scratch/yl11109/trained_models_masked_R2/checkpoint-3125\" \n",
    "\n",
    "max_seq_length = 2048  # Choose any sequence length\n",
    "dtype = None  # This will auto-detect the best data type for your GPU\n",
    "load_in_4bit = True  # Use 4-bit quantization to save memory\n",
    "# Load the model and tokenizer from the saved path\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = save_path,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqQfOSIaHxH5"
   },
   "source": [
    "# Validation loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "afSH9cHhHy-i"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# import random\n",
    "# Load the full training dataset\n",
    "full_dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split=\"train\")\n",
    "\n",
    "# Shuffle the dataset for randomness and create our smaller splits\n",
    "shuffled_dataset = full_dataset.shuffle(seed=42)\n",
    "validation_dataset = shuffled_dataset.select(range(800000,800000+2000)) \n",
    "# The instructional prompt template for validation\n",
    "validation_prompt = \"\"\"You are a great mathematician and you are tasked with finding if a solution to a given maths question is correct or not. Your response should be 'true' if the solution is correct, otherwise 'false'. Below is the Question, Solution, and the Answer.\n",
    "Question:\n",
    "{}\n",
    "Solution:\n",
    "{}\n",
    "Answer:\n",
    "{}\n",
    "Output:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgTKDx11HqwX"
   },
   "source": [
    "# Accuracy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "axwoSip1Hsp4",
    "outputId": "49899dd8-7161-43d6-e7e5-77ae3b43718a"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Prepare the model for faster inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Create the prompt template for inference (no answer included)\n",
    "validation_prompt = \"\"\"You are a great mathematician and you are tasked with finding if a solution to a given maths question is correct or not. Your response should be 'true' if the solution is correct, otherwise 'false'. Below is the Question, Solution, and the Answer.\n",
    "Question:\n",
    "{}\n",
    "Solution:\n",
    "{}\n",
    "Answer:\n",
    "{}\n",
    "Output:\n",
    "\"\"\"\n",
    "\n",
    "def parse_output(response_text):\n",
    "    # Find the text after \"Output:\"\n",
    "    output_part = response_text.split(\"Output:\\n\")[-1]\n",
    "    # Check if \"True\" is in that part, case-insensitively\n",
    "    if 'true' in output_part.lower():\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Iterate over the whole validation set and compute accuracy\n",
    "total = 0\n",
    "correct = 0\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for example in tqdm(validation_dataset):\n",
    "    question = example[\"question\"]\n",
    "    solution = example[\"solution\"]\n",
    "    answer = example[\"answer\"]\n",
    "    texts = validation_prompt.format(question, str(solution),str(answer))\n",
    "    inputs = tokenizer([texts], return_tensors=\"pt\").to(\"cuda\")\n",
    "    # Generate the model's response\n",
    "    outputs = model.generate(**inputs, max_new_tokens=8, use_cache=True)\n",
    "    response_text = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "    # Parse the prediction and add it to our list\n",
    "    prediction = str(parse_output(response_text)).strip().lower()\n",
    "\n",
    "    true_label = str(example[\"is_correct\"]).strip().lower()  # expected: \"true\" or \"false\"\n",
    "\n",
    "    predictions.append(prediction)\n",
    "    true_labels.append(true_label)\n",
    "    # Update counters\n",
    "    total += 1\n",
    "    if prediction == true_label:\n",
    "        correct += 1\n",
    "\n",
    "# Print final accuracy\n",
    "accuracy = 100.0 * correct / total if total else 0.0\n",
    "print(f\"\\nValidation: {correct}/{total} correct, Accuracy = {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMnOxkbFO_bU"
   },
   "source": [
    "# Submission csv generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kU9KMhcXO-_P",
    "outputId": "8c2ccc35-9160-408b-88f0-870a37084c13"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# Prepare the model for faster inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "# Prompt\n",
    "test_prompt = \"\"\"You are a great mathematician and you are tasked with finding if a solution to a given maths question is correct or not. Your response should be 'true' if the solution is correct, otherwise 'false'. Below is the Question, Solution, and the Answer.\n",
    "Question:\n",
    "{}\n",
    "Solution:\n",
    "{}\n",
    "Answer:\n",
    "{}\n",
    "Output:\n",
    "\"\"\"\n",
    "# Load the official test set\n",
    "test_dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split=\"test\")\n",
    "predictions = []\n",
    "\n",
    "# A simple function to parse 'True' or 'False' from the model's raw output\n",
    "def parse_output(response_text):\n",
    "    # Find the text after \"Output:\"\n",
    "    output_part = response_text.split(\"Output:\\n\")[-1]\n",
    "    # Check if \"True\" is in that part, case-insensitively\n",
    "    if 'true' in output_part.lower():\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Loop through the test dataset and generate a prediction for each example\n",
    "for example in tqdm(test_dataset):\n",
    "    question = example[\"question\"]\n",
    "    solution = example[\"solution\"]\n",
    "    answer = example[\"answer\"]\n",
    "    # Format the prompt\n",
    "    prompt = test_prompt.format(question, str(solution), str(answer))\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate the prediction\n",
    "    outputs = model.generate(**inputs, max_new_tokens=8, use_cache=True)\n",
    "    response_text = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "    # Parse the prediction and add it to our list\n",
    "    prediction = parse_output(response_text)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'ID': range(len(predictions)),\n",
    "    'is_correct': predictions\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "submission.to_csv('/scratch/yl11109/submission.csv', index=False)\n",
    "\n",
    "print(\"\\nSubmission file 'submission.csv' created successfully!\")\n",
    "print(\"You can now download this file and submit it to the Kaggle competition.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Unsloth (conda)",
   "language": "python",
   "name": "unsloth"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
